% arara: xelatex
\documentclass[12pt]{article}

\usepackage{physics}


\usepackage{tikz} % картинки в tikz
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering}

\usepackage{amsmath, amsfonts, amssymb} % куча стандартных математических плюшек

\usepackage{comment}

\usepackage[top=2cm, left=1.2cm, right=1.2cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption}

\usepackage{url} % to use \url{link to web}

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{DSE, ICEF, Fall 2023}
\chead{}
\rhead{HA-03, theory}
\lfoot{}
\cfoot{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\usepackage{tcolorbox} % рамочки!

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos - печатает все поставленные \todo'шки


% более красивые таблицы
\usepackage{booktabs}
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"



\usepackage{fontspec}
\usepackage{polyglossia}

\setmainlanguage{english}
\setotherlanguages{english}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
\setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
\newfontfamily{\cyrillicfonttt}{Linux Libertine O}

%\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
%\setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*}

%% эконометрические сокращения
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\Var}{\mathbb{V}ar}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}
% \DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\pCorr}{\mathrm{p}\mathbb{C}\mathrm{orr}}


\newcommand \hb{\hat{\beta}}
\newcommand \hs{\hat{\sigma}}
\newcommand \htheta{\hat{\theta}}
\newcommand \s{\sigma}
\newcommand \hy{\hat{y}}
\newcommand \hY{\hat{Y}}
\newcommand \e{\varepsilon}
\newcommand \he{\hat{\e}}
\newcommand \z{z}
\newcommand \hVar{\widehat{\Var}}
\newcommand \hCorr{\widehat{\Corr}}
\newcommand \hCov{\widehat{\Cov}}
\newcommand \cN{\mathcal{N}}
\newcommand \RR{\mathbb{R}}
\newcommand \NN{\mathbb{N}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\dBin}{\mathrm{Bin}}


\begin{document}



\begin{enumerate}
\item Consider the classic least squares model, $y = X\beta + u$ with $\E(u \mid X) = 0$, $\Var(u \mid X) = \sigma^2 \times I$. 
Here $I$ is $n \times n$ identity matrix, $X$ is $n \times k$ matrix with linearly independent columns. 

The estimator $\hat \beta$ is obtained by ordinary least squares, $\hat y$ is the vector of in-sample forecasts,
the residual vector is $\hat u = \hat y - y$. 

\begin{enumerate}
    \item Find $\E(\hat y \mid X)$, $\E(\hat u \mid X)$.
    \item Find $\Var(\hat u \mid X)$, $\Var(\hat y \mid X)$.
    \item Find $\Cov(\hat u, \hat \beta \mid X)$, $\Cov(\hat u, \hat y \mid X)$.
\end{enumerate}

\item Consider a simple linear regression $\hat y_i = \hat \beta_1 + \hat\beta_2 x_i$ with $L^2$ penalty,
\[
\sum_{i=1}^n (\hat y_i - y_i)^2  + \lambda \cdot \hat\beta_2^2 \to \underset{\hat\beta_1, \hat\beta_2}{\min}.
\]

\begin{enumerate}
    \item Find $\hat\beta_1$ and $\hat\beta_2$ for the penalized problem. 
    \item Prove that you can obtain exactly the same $\hat\beta_1$ and $\hat\beta_2$ 
    just by adding some special additional observations to the initial dataset and using vanilla least squares without penalty. 
    Describe these special additional observations explicitely. 
\end{enumerate}

Comment: this idea of equivalence between penalty and adding observations is widely used in bayesian vector autoregressive models.  


\item Consider the matrix $A$
\[
    A = \begin{pmatrix}
        2 & 5 \\
        2 & 5 \\
        4 & 6 \\
        4 & 7 \\
        3 & 7 \\
    \end{pmatrix}
\]

\begin{enumerate}
    \item Standardize the columns of $A$. Let's denote the new matrix by $X$.
    \item Calculate $X^TX$, its eigenvalues and eigenvectors.
    \item Calculate $XX^T$, its eigenvalues and eigenvectors.
    \item Calculate singular value decomposition of $X$. 
    \item Calculate the first principal component of $X$. 
\end{enumerate}

\end{enumerate}


\end{document}

